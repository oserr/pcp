\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{backnaur}
\usepackage[scaled]{beramono}
\usepackage{bm}
\usepackage[small,bf]{caption}
\usepackage[strict]{changepage}
\usepackage{dblfloatfix}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{flushend}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{ifsym}
\usepackage{lipsum}
\usepackage{listings}
\usepackage{makeidx}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{textcomp}
\usepackage[hyphens]{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{pgfgantt}
\usepackage{wrapfig}
\usepackage{balance}
\usepackage{tikz}
\usetikzlibrary{shapes,decorations}
\usepackage{pgfplots}
\usepgfplotslibrary{units}
\pgfplotsset{compat=1.14}
\usepackage{bm}
\usepackage[
backend=biber,
style=ieee
]{biblatex}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
%\bibliographystyle{IEEEtran}
%\bibliographystyle{acm}
\addbibresource{references.bib}

\newcommand{\rt}{\textsuperscript{\textregistered}}
\newcommand{\tm}{\texttrademark}

\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-25pt}
\huge CS 15-618 Project Proposal \\
\huge Synchrony
}
\author{
    Patricio Chilano (pchilano) \\
    Omar Serrano (oserrano)
}
\date{\today}

\begin{document}

\definecolor{beaublue}{rgb}{0.74, 0.83, 0.9}

\lstset{
    language=C++,
    basicstyle=\ttfamily\scriptsize,
    keywordstyle=\color{blue}\ttfamily,
    stringstyle=\color{red}\ttfamily,
    commentstyle=\color{orange}\ttfamily,
    morecomment=[l][\color{magenta}]{\#},
    breaklines=true
}

\maketitle

\section*{Summary}
We are going to implement a concurrent doubly-linked list, and a hash map,
lock-free, and with coarse-grained and fine-grained locks, in order to
investigate how these concurrent data structures are implemented, and to study
first-hand how these techniques compare with each other on a variety of
different workloads.

\section*{Background}
Even though some parallel applications may get away without needing concurrent
data structures, arguably the more common paradigm is for threaded applications
to use them. Work queues, for example, are a good way for a server to model the
producer-consumer paradigm, as the server puts newly created connections into a
queue, and a pool of threads take items from the queue as they are added to it.
A parallel implementation of $A^*$, for example, might need a concurrent
priority queue for nodes in the frontier, and a concurrent hash set to record
the nodes that have already been visited.

The common theme of these examples is the need for thread-safe data structures,
yet both present different use cases of concurrency, and hence the best type of
synchronization mechanism used by each application is different. The server is
well served by a blocking queue, because it may go through periods without
client requests. While it waits, the server can free up the CPU for other tasks.
If it is part of a distributed application, for example, it can send heartbeats
to other hosts, create a checkpoint, etc.

On the other hand, when $A^*$ runs it will simply try to process a graph as
quickly as possible in order to find the best path from one node to the other.
Therefore, a parallel version of $A^*$ that uses blocking concurrent data
structures may perform more poorly than one with lock-free data structures. When
a thread pops a node from the priority queue of nodes in the frontier, i.e.,
nodes yet-to-be explored, or inserts a node into the hash set of visited nodes,
it does quickly. If $A^*$ uses blocking concurrent data structures, then the
cost of thread preemption and rescheduling by the OS is likely to dominate the
cost of synchronizing access to the data structures.

As these examples demonstrate, different applications are better suited for
different forms of synchronization. In addition to the particulars of an
application, other factors may affect the behavior of concurrent data structures
implemented with different synchronizatin mechanisms, such as the workload or
the number of processors and threads used.

\section*{Challenge}

\section*{Platform Choice}
We are using C/C++ to implement the concurrent data structures. We believe this
is the best choice in terms of the flexibility it gives us in terms of design
choices, and it is almost a necessary choice when it comes to lock-free
mechanisms, because the language needs to provide an interface for direct memory
access. There are other programming languages that will also provide direct
memory access, such as \href{https://golang.org/}{golang} and
\href{https://www.rust-lang.org/en-US/}{rust}, but C/C++ also provides a direct
interface to the OS threading infrastructure, i.e., \texttt{Pthreads}, thus
allowing us to tinker with the system at the lowest level of the software/hardware
interface. More specifically, we are planning to use the C++11 standard, which
introduced threads to C++, and we'll be making heavy use of the tools in
\href{http://en.cppreference.com/w/cpp/header/thread}{\texttt{<thread>}},
\href{http://en.cppreference.com/w/cpp/header/mutex}{\texttt{<mutex>}}, and
\href{http://en.cppreference.com/w/cpp/header/atomic}{\texttt{<atomic>}}.

We are planning to run all of our experiments on Posix OSes, such as Linux or
macOS, running x86\_64. The x86\_64 architecture is more modern than other
system architectures and we may run into a problem where we need a
synchronization or memory model feature only found in this architecture. Linux
and macOS are good choices for us to implement our data structures, because they
are Posix OSes, provide the familiar \texttt{Pthreads} abstraction/interface,
and are programmer-friendly. Furthermore, these platforms are the most
convenient for us, because GHC machines and the latedays cluster are Linux
x86\_64, and our laptops are macOSes x86\_64.

We are also planning to use Java in order to compare the performance of our data structures with Java's
\href{https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentHashMap.html}{ConcurrentHashMap} and
\href{https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentLinkedDeque.html}{ConcurrentLinkedDeque}.
Despite the differences between Java and C++, these Java concurrent data
structures are a good choice to compare with our library, because Java's
concurrent library is a mature library, is well known, and it is implemented by
experts in their field.

\section*{Resources}

\subsection*{Code/Libraries/Compilers}
The point of our project is to implement concurrent data structures, so we'll
build our library from scratch. Given that we'll compare our library with Java's
concurrent package on our macOSes, the GHC machines, and the latedays cluster,
we'll use Java 8. To test our data structures for correctness, we will use the
\href{https://github.com/google/googletest}{GoogleTest} library for unit testing.
Per Table~\ref{table:compiler}, the machines we'll be using have different
compiler and Java runtime environments.

\begin{table}[t]
\begin{center}
\begin{tabular}{llll}
\toprule
\bf Machine(s) & \bf C++ & \bf Java \\
\midrule
GHC            & gcc 4.9.2 & OpenJDK 8 \\
latedays       & gcc 4.9.2 & Oracle 8 \\
macOS          & gcc 6.4 & Oracle 10 \\
\bottomrule
\end{tabular}
\caption{Compiler and Java RE toolchains we will use for our experiments.}
\label{table:compiler}
\end{center}
\end{table}

% machines
\subsection*{Machines}
We've yet to choose the machines where we'll carry our experiments, but we'll
probably settle on one GHC machine, a machine on the latedays cluster (via
\texttt{qsub -q @server}), one of our laptops, and, if time permits, the Phi
Cores. These are a good sample of machines to test our concurrent data
structures, because they have different specs and this will allow us to see how
the data structures scale and perform under different conditions/environments.
The specs for these machines are listed in Table~\ref{table:specs}. Particular
points of interest specific to the specs of the machines we are planning to use
are:

\begin{itemize}
\item
macOS has the least number of cores, but it has the most modern and powerful
cores. How do our data structures scale from 1 to 8 threads on the macOS
compared with the other machines?
\item
GHC machines have 16 virtual cores, more than the mac, but less than machines in
the latedays cluster; however, machines in the latedays cluster are dual socket,
whereas GHC machines are single socket. How does this affect the scaling
behavior, in particular when ramping up the number of threads from 12 to 16 cores?
\item
At 61 cores, the Phi Cores offer, by far, the highest level of parallelism and
number of threads. How do our data structures scale at this level of parallelism?
\end{itemize}

\begin{table}[t]
\begin{center}
\begin{tabular}{llll}
\toprule
\bf Machine(s) & \bf Specs & \bf Cores & \bf Threads   \\
\midrule
GHC            & Intel\rt Xeon\rt CPU E5-1660 v4 @ 3.20GHz & 8 & 16 \\
latedays       & Intel\rt Xeon\rt CPU E5-2620 v3 @ 2.40GHz & 6 & 12 \\
macOS          & Intel\rt Core\tm i7-7700HQ @ 2.80GHz & 4 & 8 \\
Phi Cores      & SomeSpecs & Some\# & Some\# \\
\bottomrule
\end{tabular}
\caption{Specs of machines we plan to use for experiments.}
\label{table:specs}
\end{center}
\end{table}

\subsection*{Papers/Book}
We will start our research into the implementation of high performance
concurrent data structures with \cite{Harris, Fomitchev, Maged, Williams}.
\cite{Harris, Fomitchev} seem to focus exclusively on non-blocking
lists, but whatever we learn here can also be applied to a concurrent hash map,
because a hash map is built-up as buckets of lists; however, \cite{Maged} delves
into the topic of lock-free hash tables, so we'll be able to get more insights
about how to build a lock-free hash map from lock-free linked lists.
\cite{Willains} is a tutorial book, and it will be very useful to us because it
focuses explicitly on using the threading primitives provided by C++11 to build
concurrent application, and it includes examples of fine-grained blocking and
non-blocking data structures.

\section*{Goals and Deliverables}

\begin{itemize}
\item Doubly linked list (DLL) with
\begin{itemize}
\item coarse-grained locks
\item fined-grained locks
\item lock-free
\end{itemize}
\item Test harness for linked list
\item Hash map (HM) with
\begin{itemize}
\item coarse-grained locks
\item fined-grained locks
\item lock-free
\end{itemize}
\item test harness for hash map
\end{itemize}

\begin{table}[t]
\begin{center}
\begin{tabular}{ll}
\toprule
\bf Week & \bf Deliverable   \\
\midrule
4/9      & DLL with coarse grained locks \\
         & DLL Test harness \\
4/16     & Check point 1 \\
         & DLL fined-grained locks \\
         & HM coarse-grained locks \\
         & HM test-harness \\
4/23     & Check point 2 \\
         & DLL lock-free \\
         & HM fine-grained \\
4/30     & HM lock-free \\
         & Report \\
\bottomrule
\end{tabular}
\caption{Weekly Schedule}
\label{table:shedule}
\end{center}
\end{table}

\section*{Schedule}

\printbibliography

\end{document}
